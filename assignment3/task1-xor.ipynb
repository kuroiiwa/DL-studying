{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy.random import shuffle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot configurations\n",
    "% matplotlib inline\n",
    "\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1, Part 1: Backpropagation through time (BPTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Consider a simple RNN network shown in the following figure, where _wx, wh, b1, w, b2_ are the scalar parameters of the network. The loss function is the **mean squared error (MSE)**. Given input _(x1, x2) = (1, 0)_, ground truth _(g1, g2) = (1, 1), h0 = 0, (wx, wh, b1, w, b2) = (1, 1, 1, 1, 1)_, compute _(dwx, dwh, db1, dw, db2)_, which are the gradients of loss with repect to 5 parameters _(wx, wh, b1, w, b2)_.\n",
    "\n",
    "![bptt](./img/bptt2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">TODO:</span>\n",
    "\n",
    "Answer the above question. \n",
    "\n",
    "* **[fill in here: Enter your derivations and the computational process]**\n",
    "* You can use LATEX to edit the equations, and Jupyter notebook can recognize basic LATEX syntax. Alternatively, you can edit equations in some other environment and then paste the screenshot of the equations here.\n",
    "<br>\n",
    "Loss: $$L = \\frac{1}{2}\\sum_{t=1}^{2}L_t=\\frac{1}{2}((y_1-g_1)^2+(y_2-g_2)^2)$$\n",
    "\n",
    "dw: $$\\frac{\\partial L}{\\partial w}=\\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial w}=\\sum_{t=1}^{2}(y_t-g_t)(1-y_t)y_th_t \\approx -0.0263$$\n",
    "\n",
    "db2: $$\\frac{\\partial L}{\\partial b2}=\\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial b2}=\\sum_{t=1}^{2}(y_t-g_t)(1-y_t)y_t \\approx -0.0288$$\n",
    "\n",
    "dwx: $$\\frac{\\partial L}{\\partial wx}=\\frac{\\partial L_1}{\\partial y_1}\\frac{\\partial y_1}{\\partial h_1}\\frac{\\partial h_1}{\\partial wx} + \\frac{\\partial L_2}{\\partial y_2}\\frac{\\partial y_2}{\\partial h_2}\\frac{\\partial h_2}{\\partial h_1}\\frac{\\partial h_1}{\\partial wx} \\approx -0.001595$$\n",
    "\n",
    "dwh: $$\\frac{\\partial L}{\\partial wh}=\\frac{\\partial L_1}{\\partial y_1}\\frac{\\partial y_1}{\\partial h_1}\\frac{\\partial h_1}{\\partial wh} + \\frac{\\partial L_2}{\\partial y_2}\\frac{\\partial y_2}{\\partial h_2}\\frac{\\partial h_2}{\\partial h_1}\\frac{\\partial h_1}{\\partial wh} = 0$$\n",
    "\n",
    "db1: $$\\frac{\\partial L}{\\partial wh}=\\frac{\\partial L_1}{\\partial y_1}\\frac{\\partial y_1}{\\partial h_1}\\frac{\\partial h_1}{\\partial b1} + \\frac{\\partial L_2}{\\partial y_2}\\frac{\\partial y_2}{\\partial h_2}\\frac{\\partial h_2}{\\partial h_1}\\frac{\\partial h_1}{\\partial b1} \\approx -0.001595$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1, Part 2: Use tensorflow modules to create XOR network\n",
    "\n",
    "In this part, you need to build and train an XOR network that can learn the XOR function. It is a very simple implementation of RNN and will give you an idea how RNN is built and how to train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR network\n",
    "\n",
    "XOR network can learn the XOR $\\oplus$ function\n",
    "\n",
    "As shown in the figure below, and for instance, if input $(x0, x1, x2)$=(1,0,0), then output $(y1, y2, y3)$=(1,1,1). That is, $y_n = x_0\\oplus x_1 \\oplus ... \\oplus x_{n-1}$\n",
    "\n",
    "![xor_net](./img/xor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data set\n",
    "This function provides you the way to generate the data which is required for the training process. You should utilize it when building your training function for the GRU. Please read the source code for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ecbm4040.xor.utils import create_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a network using a Tensorlow GRUCell\n",
    "This section shows an example how to build a RNN network using an GRU cell. GRU cell is an inbuilt class in tensorflow which implements the real behavior of the GRU neuron. \n",
    "\n",
    "Reference: \n",
    "1. [TensorFlow GRU cell](https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/contrib/rnn/GRUCell)\n",
    "2. [Understanding GRU networks](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import GRUCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Input shape: (num_samples, seq_length, input_dimension)\n",
    "# Output shape: (num_samples, output_ground_truth), and output_ground_truth is 0/1.\n",
    "input_data = tf.placeholder(tf.float32, shape=[None,None,1])\n",
    "output_data = tf.placeholder(tf.int64, shape=[None,None])\n",
    "\n",
    "# define GRU cell\n",
    "num_units = 64\n",
    "cell = GRUCell(num_units)\n",
    "\n",
    "# create GRU network: you can also choose other modules provided by tensorflow, like static_rnn etc.\n",
    "hidden, _ = tf.nn.dynamic_rnn(cell, input_data, dtype=tf.float32)\n",
    "\n",
    "# generate output from the hidden information\n",
    "output_shape = 2\n",
    "out = tf.layers.dense(hidden, output_shape)\n",
    "pred = tf.argmax(out, axis=2)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=output_data,logits=out))\n",
    "\n",
    "# optimization\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.1).minimize(loss)\n",
    "\n",
    "# accuracy\n",
    "correct_num = tf.equal(output_data,pred)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_num,tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n",
    "\n",
    "<span style='color:red'>TODO:</span> \n",
    "1. Build your training funciton for RNN; \n",
    "2. Plot the cost during the traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TRAINING AND PLOTTING CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task 1, Part 3 :  Build your own GRUCell\n",
    "In this part, you need to build your own GRU cell to achieve the GRU functionality. \n",
    "\n",
    "<span style=\"color:red\">TODO:</span> \n",
    "1. Finish class **MyGRUCell** in ecbm4040/xor/rnn.py;\n",
    "2. Write the training function for your RNN;\n",
    "3. Plot the cost during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ecbm4040.xor.rnn import MyGRUCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# recreate xor netowrk with your own GRU cell\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Input shape: (num_samples,seq_length,input_dimension)\n",
    "#Output shape: (num_samples, output_ground_truth), and output_ground_truth is 0/1. \n",
    "input_data = tf.placeholder(tf.float32,shape=[None,None,1])\n",
    "output_data = tf.placeholder(tf.int64,shape=[None,None])\n",
    "\n",
    "# recreate xor netowrk with your own GRU cell\n",
    "num_units = 64\n",
    "cell = MyGRUCell(num_units)\n",
    "\n",
    "# create GRU network: you can also choose other modules provided by tensorflow, like static_rnn etc.\n",
    "hidden, _ = tf.nn.dynamic_rnn(cell,input_data,dtype=tf.float32)\n",
    "\n",
    "# generate output from the hidden information\n",
    "output_shape = 2\n",
    "out = tf.layers.dense(hidden, output_shape)\n",
    "pred = tf.argmax(out,axis=2)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=output_data,logits=out))\n",
    "# optimization\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.1).minimize(loss)\n",
    "# accuracy\n",
    "correct = tf.equal(output_data,pred)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TRAINING AND PLOTTING CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
